{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["Vqx02gK4R_fd","WfvCcX_cWFup","U1MvptGi7QFv","YKtP3CtW_1_k","0mPkYVvk-W1F","hegow6PdJfQZ"],"authorship_tag":"ABX9TyOT59x1FfP9AfI+NKFVXg7g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Downloading Dataset**"],"metadata":{"id":"Vqx02gK4R_fd"}},{"cell_type":"code","source":["dataset_folder_name = \"Dataset\""],"metadata":{"id":"GuiM2aVdUx-6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import auth\n","auth.authenticate_user()"],"metadata":{"id":"lqaW5KujSCHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n","!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n","!apt -qq update\n","!apt -qq install gcsfuse"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Lq1XIfwSC2V","executionInfo":{"status":"ok","timestamp":1688065178746,"user_tz":-210,"elapsed":12378,"user":{"displayName":"AIA Team","userId":"16477440184016990512"}},"outputId":"098a9318-0ddf-43aa-a473-185f84147832"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2659  100  2659    0     0  40907      0 --:--:-- --:--:-- --:--:-- 40907\n","OK\n","20 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","The following NEW packages will be installed:\n","  gcsfuse\n","0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n","Need to get 14.0 MB of archives.\n","After this operation, 31.2 MB of additional disk space will be used.\n","Selecting previously unselected package gcsfuse.\n","(Reading database ... 123069 files and directories currently installed.)\n","Preparing to unpack .../gcsfuse_1.0.0_amd64.deb ...\n","Unpacking gcsfuse (1.0.0) ...\n","Setting up gcsfuse (1.0.0) ...\n"]}]},{"cell_type":"code","source":["!mkdir $dataset_folder_name\n","!gcsfuse --implicit-dirs i-care-2.0.physionet.org $dataset_folder_name"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sz-8Ptn2TogE","executionInfo":{"status":"ok","timestamp":1688065179814,"user_tz":-210,"elapsed":1094,"user":{"displayName":"AIA Team","userId":"16477440184016990512"}},"outputId":"50cd0693-431a-49c8-bc8a-9fef5f4b6fc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I0629 18:59:38.406204 2023/06/29 18:59:38.406170 Start gcsfuse/1.0.0 (Go version go1.20.4) for app \"\" using mount point: /content/Dataset\n"]}]},{"cell_type":"markdown","source":["# **helper_code.py**"],"metadata":{"id":"WfvCcX_cWFup"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","\n","# Do *not* edit this script.\n","# These are helper functions that you can use with your code.\n","# Check the example code to see how to import these functions to your code.\n","\n","import os, numpy as np, scipy as sp, scipy.io\n","\n","### Challenge data I/O functions\n","\n","# Find the folders with data files.\n","def find_data_folders(root_folder):\n","    data_folders = list()\n","    for x in sorted(os.listdir(root_folder)):\n","        data_folder = os.path.join(root_folder, x)\n","        if os.path.isdir(data_folder):\n","            data_file = os.path.join(data_folder, x + '.txt')\n","            if os.path.isfile(data_file):\n","                data_folders.append(x)\n","    return sorted(data_folders)\n","\n","# Load the patient metadata: age, sex, etc.\n","def load_challenge_data(data_folder, patient_id):\n","    patient_metadata_file = os.path.join(data_folder, patient_id, patient_id + '.txt')\n","    patient_metadata = load_text_file(patient_metadata_file)\n","    return patient_metadata\n","\n","# Find the record names.\n","def find_recording_files(data_folder, patient_id):\n","    record_names = list()\n","    patient_folder = os.path.join(data_folder, patient_id)\n","    for file_name in sorted(os.listdir(patient_folder)):\n","        if not file_name.startswith('.') and file_name.endswith('.hea'):\n","            root, ext = os.path.splitext(file_name)\n","            record_name = '_'.join(root.split('_')[:-1])\n","            record_names.append(record_name)\n","    return sorted(record_names)\n","\n","# Load the WFDB data for the Challenge (but not all possible WFDB files).\n","def load_recording_data(record_name, check_values=True):\n","    # Allow either the record name or the header filename.\n","    root, ext = os.path.splitext(record_name)\n","    if ext=='':\n","        header_file = record_name + '.hea'\n","    else:\n","        header_file = record_name\n","\n","    # Load the header file.\n","    if not os.path.isfile(header_file):\n","        raise FileNotFoundError('{} recording not found.'.format(record_name))\n","\n","    with open(header_file, 'r') as f:\n","        header = [l.strip() for l in f.readlines() if l.strip()]\n","\n","    # Parse the header file.\n","    record_name = None\n","    num_signals = None\n","    sampling_frequency = None\n","    num_samples = None\n","    signal_files = list()\n","    gains = list()\n","    offsets = list()\n","    channels = list()\n","    initial_values = list()\n","    checksums = list()\n","\n","    for i, l in enumerate(header):\n","        arrs = [arr.strip() for arr in l.split(' ')]\n","        # Parse the record line.\n","        if i==0:\n","            record_name = arrs[0]\n","            num_signals = int(arrs[1])\n","            sampling_frequency = float(arrs[2])\n","            num_samples = int(arrs[3])\n","        # Parse the signal specification lines.\n","        elif not l.startswith('#') or len(l.strip()) == 0:\n","            signal_file = arrs[0]\n","            gain = float(arrs[2].split('/')[0])\n","            offset = int(arrs[4])\n","            initial_value = int(arrs[5])\n","            checksum = int(arrs[6])\n","            channel = arrs[8]\n","            signal_files.append(signal_file)\n","            gains.append(gain)\n","            offsets.append(offset)\n","            initial_values.append(initial_value)\n","            checksums.append(checksum)\n","            channels.append(channel)\n","\n","    # Check that the header file only references one signal file. WFDB format allows for multiple signal files, but, for\n","    # simplicity, we have not done that here.\n","    num_signal_files = len(set(signal_files))\n","    if num_signal_files!=1:\n","        raise NotImplementedError('The header file {}'.format(header_file) \\\n","            + ' references {} signal files; one signal file expected.'.format(num_signal_files))\n","\n","    # Load the signal file.\n","    head, tail = os.path.split(header_file)\n","    signal_file = os.path.join(head, list(signal_files)[0])\n","    data = np.asarray(sp.io.loadmat(signal_file)['val'])\n","\n","    # Check that the dimensions of the signal data in the signal file is consistent with the dimensions for the signal data given\n","    # in the header file.\n","    num_channels = len(channels)\n","    if np.shape(data)!=(num_channels, num_samples):\n","        raise ValueError('The header file {}'.format(header_file) \\\n","            + ' is inconsistent with the dimensions of the signal file.')\n","\n","    # Check that the initial value and checksums in the signal file are consistent with the initial value and checksums in the\n","    # header file.\n","    if check_values:\n","        for i in range(num_channels):\n","            if data[i, 0]!=initial_values[i]:\n","                raise ValueError('The initial value in header file {}'.format(header_file) \\\n","                    + ' is inconsistent with the initial value for channel {} in the signal data'.format(channels[i]))\n","            if np.sum(data[i, :])!=checksums[i]:\n","                raise ValueError('The checksum in header file {}'.format(header_file) \\\n","                    + ' is inconsistent with the checksum value for channel {} in the signal data'.format(channels[i]))\n","\n","    # Rescale the signal data using the gains and offsets.\n","    rescaled_data = np.zeros(np.shape(data), dtype=np.float32)\n","    for i in range(num_channels):\n","        rescaled_data[i, :] = (data[i, :]-offsets[i])/gains[i]\n","\n","    return rescaled_data, channels, sampling_frequency\n","\n","# Choose the channels.\n","def reduce_channels(current_data, current_channels, requested_channels):\n","    if current_channels == requested_channels:\n","        reduced_data = current_data\n","        reduced_channels = current_channels\n","    else:\n","        reduced_indices = [current_channels.index(channel) for channel in requested_channels if channel in current_channels]\n","        reduced_channels = [current_channels[i] for i in reduced_indices]\n","        reduced_data = current_data[reduced_indices, :]\n","    return reduced_data, reduced_channels\n","\n","# Choose the channels.\n","def expand_channels(current_data, current_channels, requested_channels):\n","    if current_channels == requested_channels:\n","        expanded_data = current_data\n","    else:\n","        num_current_channels, num_samples = np.shape(current_data)\n","        num_requested_channels = len(requested_channels)\n","        expanded_data = np.zeros((num_requested_channels, num_samples))\n","        for i, channel in enumerate(requested_channels):\n","            if channel in current_channels:\n","                j = current_channels.index(channel)\n","                expanded_data[i, :] = current_data[j, :]\n","            else:\n","                expanded_data[i, :] = float('nan')\n","    return expanded_data\n","\n","### Helper Challenge data I/O functions\n","\n","# Load text file as a string.\n","def load_text_file(filename):\n","    with open(filename, 'r') as f:\n","        data = f.read()\n","    return data\n","\n","# Get a variable from the patient metadata.\n","def get_variable(text, variable_name, variable_type):\n","    variable = None\n","    for l in text.split('\\n'):\n","        if l.startswith(variable_name):\n","            variable = ':'.join(l.split(':')[1:]).strip()\n","            variable = cast_variable(variable, variable_type)\n","            return variable\n","\n","# Get the patient ID variable from the patient data.\n","def get_patient_id(string):\n","    return get_variable(string, 'Patient', str)\n","\n","# Get the patient ID variable from the patient data.\n","def get_hospital(string):\n","    return get_variable(string, 'Hospital', str)\n","\n","# Get the age variable (in years) from the patient data.\n","def get_age(string):\n","    return get_variable(string, 'Age', int)\n","\n","# Get the sex variable from the patient data.\n","def get_sex(string):\n","    return get_variable(string, 'Sex', str)\n","\n","# Get the ROSC variable (in minutes) from the patient data.\n","def get_rosc(string):\n","    return get_variable(string, 'ROSC', int)\n","\n","# Get the OHCA variable from the patient data.\n","def get_ohca(string):\n","    return get_variable(string, 'OHCA', bool)\n","\n","# Get the shockable rhythm variable from the patient data.\n","def get_shockable_rhythm(string):\n","    return get_variable(string, 'Shockable Rhythm', bool)\n","\n","# Get the TTM variable (in Celsius) from the patient data.\n","def get_ttm(string):\n","    return get_variable(string, 'TTM', int)\n","\n","# Get the Outcome variable from the patient data.\n","def get_outcome(string):\n","    variable = get_variable(string, 'Outcome', str)\n","    if variable is None or is_nan(variable):\n","        raise ValueError('No outcome available. Is your code trying to load labels from the hidden data?')\n","    if variable == 'Good':\n","        variable = 0\n","    elif variable == 'Poor':\n","        variable = 1\n","    return variable\n","\n","# Get the Outcome probability variable from the patient data.\n","def get_outcome_probability(string):\n","    variable = sanitize_scalar_value(get_variable(string, 'Outcome Probability', str))\n","    if variable is None or is_nan(variable):\n","        raise ValueError('No outcome available. Is your code trying to load labels from the hidden data?')\n","    return variable\n","\n","# Get the CPC variable from the patient data.\n","def get_cpc(string):\n","    variable = sanitize_scalar_value(get_variable(string, 'CPC', str))\n","    if variable is None or is_nan(variable):\n","        raise ValueError('No CPC score available. Is your code trying to load labels from the hidden data?')\n","    return variable\n","\n","# Get the utility frequency (in Hertz) from the recording data.\n","def get_utility_frequency(string):\n","    return get_variable(string, '#Utility frequency', int)\n","\n","# Get the start time (in hh:mm:ss format) from the recording data.\n","def get_start_time(string):\n","    variable = get_variable(string, '#Start time', str)\n","    times = tuple(int(value) for value in variable.split(':'))\n","    return times\n","\n","# Get the end time (in hh:mm:ss format) from the recording data.\n","def get_end_time(string):\n","    variable = get_variable(string, '#End time', str)\n","    times = tuple(int(value) for value in variable.split(':'))\n","    return times\n","\n","# Convert seconds to days, hours, minutes, seconds.\n","def convert_seconds_to_hours_minutes_seconds(seconds):\n","    hours = int(seconds/3600 - 24*days)\n","    minutes = int(seconds/60 - 24*60*days - 60*hours)\n","    seconds = int(seconds - 24*3600*days - 3600*hours - 60*minutes)\n","    return hours, minutes, seconds\n","\n","# Convert hours, minutes, and seconds to seconds.\n","def convert_hours_minutes_seconds_to_seconds(hours, minutes, seconds):\n","    return 3600*hours + 60*minutes + seconds\n","\n","### Challenge label and output I/O functions\n","\n","\n","# Save the Challenge outputs for one file.\n","def save_challenge_outputs(filename, patient_id, outcome, outcome_probability, cpc):\n","    # Sanitize values, e.g., in case they are a singleton array.\n","    outcome = sanitize_boolean_value(outcome)\n","    outcome_probability = sanitize_scalar_value(outcome_probability)\n","    cpc = sanitize_scalar_value(cpc)\n","\n","    # Format Challenge outputs.\n","    patient_string = 'Patient: {}'.format(patient_id)\n","    if outcome == 0:\n","        outcome = 'Good'\n","    elif outcome == 1:\n","        outcome = 'Poor'\n","    outcome_string = 'Outcome: {}'.format(outcome)\n","    outcome_probability_string = 'Outcome Probability: {:.3f}'.format(outcome_probability)\n","    cpc_string = 'CPC: {:.3f}'.format(cast_int_if_int_else_float(cpc))\n","    output_string = patient_string + '\\n' + \\\n","        outcome_string + '\\n' + outcome_probability_string + '\\n' + cpc_string + '\\n'\n","\n","    # Write the Challenge outputs.\n","    if filename is not None:\n","        with open(filename, 'w') as f:\n","            f.write(output_string)\n","\n","    return output_string\n","\n","### Other helper functions\n","\n","# Check if a variable is a number or represents a number.\n","def is_number(x):\n","    try:\n","        float(x)\n","        return True\n","    except (ValueError, TypeError):\n","        return False\n","\n","# Check if a variable is an integer or represents an integer.\n","def is_integer(x):\n","    if is_number(x):\n","        return float(x).is_integer()\n","    else:\n","        return False\n","\n","# Check if a variable is a boolean or represents a boolean.\n","def is_boolean(x):\n","    if (is_number(x) and float(x)==0) or (remove_extra_characters(x) in ('False', 'false', 'FALSE', 'F', 'f')):\n","        return True\n","    elif (is_number(x) and float(x)==1) or (remove_extra_characters(x) in ('True', 'true', 'TRUE', 'T', 't')):\n","        return True\n","    else:\n","        return False\n","\n","# Check if a variable is a finite number or represents a finite number.\n","def is_finite_number(x):\n","    if is_number(x):\n","        return np.isfinite(float(x))\n","    else:\n","        return False\n","\n","# Check if a variable is a NaN (not a number) or represents a NaN.\n","def is_nan(x):\n","    if is_number(x):\n","        return np.isnan(float(x))\n","    else:\n","        return False\n","\n","# Remove any quotes, brackets (for singleton arrays), and/or invisible characters.\n","def remove_extra_characters(x):\n","    return str(x).replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '').replace(' ', '').strip()\n","\n","# Sanitize boolean values.\n","def sanitize_boolean_value(x):\n","    x = remove_extra_characters(x)\n","    if (is_number(x) and float(x)==0) or (remove_extra_characters(x) in ('False', 'false', 'FALSE', 'F', 'f')):\n","        return 0\n","    elif (is_number(x) and float(x)==1) or (remove_extra_characters(x) in ('True', 'true', 'TRUE', 'T', 't')):\n","        return 1\n","    else:\n","        return float('nan')\n","\n","# Sanitize integer values.\n","def sanitize_integer_value(x):\n","    x = remove_extra_characters(x)\n","    if is_integer(x):\n","        return int(float(x))\n","    else:\n","        return float('nan')\n","\n","# Sanitize scalar values.\n","def sanitize_scalar_value(x):\n","    x = remove_extra_characters(x)\n","    if is_number(x):\n","        return float(x)\n","    else:\n","        return float('nan')\n","\n","# Cast a value to a particular type.\n","def cast_variable(variable, variable_type, preserve_nan=True):\n","    if preserve_nan and is_nan(variable):\n","        variable = float('nan')\n","    else:\n","        if variable_type == bool:\n","            variable = sanitize_boolean_value(variable)\n","        elif variable_type == int:\n","            variable = sanitize_integer_value(variable)\n","        elif variable_type == float:\n","            variable = sanitize_scalar_value(variable)\n","        else:\n","            variable = variable_type(variable)\n","    return variable\n","\n","# Cast a value to an integer if the value is an integer, a float if the value is a non-integer float, and itself otherwise.\n","def cast_int_if_int_else_float(x):\n","    if is_integer(x):\n","        return int(float(x))\n","    elif is_number(x):\n","        return float(x)\n","    else:\n","        return x\n"],"metadata":{"id":"p50S84SGfu6r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Teamcode Helper for train**"],"metadata":{"id":"U1MvptGi7QFv"}},{"cell_type":"code","source":["import joblib\n","def save_challenge_model(model_folder, imputer, outcome_model, cpc_model):\n","    d = {'imputer': imputer, 'outcome_model': outcome_model, 'cpc_model': cpc_model}\n","    filename = os.path.join(model_folder, 'models.sav')\n","    joblib.dump(d, filename, protocol=0)"],"metadata":{"id":"JYgc8pmq8NMt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q mne\n","import mne"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2G_NfS7A7yLt","executionInfo":{"status":"ok","timestamp":1688065189706,"user_tz":-210,"elapsed":9897,"user":{"displayName":"AIA Team","userId":"16477440184016990512"}},"outputId":"1a061360-5dcc-4aab-ef3b-fb398ad237fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/7.7 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/7.7 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m7.1/7.7 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["# Preprocess data.\n","def preprocess_data(data, sampling_frequency, utility_frequency):\n","    # Define the bandpass frequencies.\n","    passband = [0.1, 30.0]\n","\n","    # Promote the data to double precision because these libraries expect double precision.\n","    data = np.asarray(data, dtype=np.float64)\n","\n","    # If the utility frequency is between bandpass frequencies, then apply a notch filter.\n","    if utility_frequency is not None and passband[0] <= utility_frequency <= passband[1]:\n","        data = mne.filter.notch_filter(data, sampling_frequency, utility_frequency, n_jobs=4, verbose='error')\n","\n","    # Apply a bandpass filter.\n","    data = mne.filter.filter_data(data, sampling_frequency, passband[0], passband[1], n_jobs=4, verbose='error')\n","\n","    # Resample the data.\n","    if sampling_frequency % 2 == 0:\n","        resampling_frequency = 128\n","    else:\n","        resampling_frequency = 125\n","    lcm = np.lcm(int(round(sampling_frequency)), int(round(resampling_frequency)))\n","    up = int(round(lcm / sampling_frequency))\n","    down = int(round(lcm / resampling_frequency))\n","    resampling_frequency = sampling_frequency * up / down\n","    data = scipy.signal.resample_poly(data, up, down, axis=1)\n","\n","    # Scale the data to the interval [-1, 1].\n","    min_value = np.min(data)\n","    max_value = np.max(data)\n","    if min_value != max_value:\n","        data = 2.0 / (max_value - min_value) * (data - 0.5 * (min_value + max_value))\n","    else:\n","        data = 0 * data\n","\n","    return data, resampling_frequency\n","\n","# Extract features.\n","def get_features(data_folder, patient_id):\n","    # Load patient data.\n","    patient_metadata = load_challenge_data(data_folder, patient_id)\n","    recording_ids = find_recording_files(data_folder, patient_id)\n","    num_recordings = len(recording_ids)\n","\n","    # Extract patient features.\n","    patient_features = get_patient_features(patient_metadata)\n","\n","    # Extract EEG features.\n","    eeg_channels = ['F3', 'P3', 'F4', 'P4']\n","    group = 'EEG'\n","\n","    if num_recordings > 0:\n","        recording_id = recording_ids[-1]\n","        recording_location = os.path.join(data_folder, patient_id, '{}_{}'.format(recording_id, group))\n","        if os.path.exists(recording_location + '.hea'):\n","            data, channels, sampling_frequency = load_recording_data(recording_location)\n","            utility_frequency = get_utility_frequency(recording_location + '.hea')\n","\n","            if all(channel in channels for channel in eeg_channels):\n","                data, channels = reduce_channels(data, channels, eeg_channels)\n","                data, sampling_frequency = preprocess_data(data, sampling_frequency, utility_frequency)\n","                data = np.array([data[0, :] - data[1, :], data[2, :] - data[3, :]]) # Convert to bipolar montage: F3-P3 and F4-P4\n","                eeg_features = get_eeg_features(data, sampling_frequency).flatten()\n","            else:\n","                eeg_features = float('nan') * np.ones(8) # 2 bipolar channels * 4 features / channel\n","        else:\n","            eeg_features = float('nan') * np.ones(8) # 2 bipolar channels * 4 features / channel\n","    else:\n","        eeg_features = float('nan') * np.ones(8) # 2 bipolar channels * 4 features / channel\n","\n","    # Extract ECG features.\n","    ecg_channels = ['ECG', 'ECGL', 'ECGR', 'ECG1', 'ECG2']\n","    group = 'ECG'\n","\n","    if num_recordings > 0:\n","        recording_id = recording_ids[0]\n","        recording_location = os.path.join(data_folder, patient_id, '{}_{}'.format(recording_id, group))\n","        if os.path.exists(recording_location + '.hea'):\n","            data, channels, sampling_frequency = load_recording_data(recording_location)\n","            utility_frequency = get_utility_frequency(recording_location + '.hea')\n","\n","            data, channels = reduce_channels(data, channels, ecg_channels)\n","            data, sampling_frequency = preprocess_data(data, sampling_frequency, utility_frequency)\n","            features = get_ecg_features(data)\n","            ecg_features = expand_channels(features, channels, ecg_channels).flatten()\n","        else:\n","            ecg_features = float('nan') * np.ones(10) # 5 channels * 2 features / channel\n","    else:\n","        ecg_features = float('nan') * np.ones(10) # 5 channels * 2 features / channel\n","\n","    # Extract features.\n","    return np.hstack((patient_features, eeg_features, ecg_features))\n","\n","# Extract patient features from the data.\n","def get_patient_features(data):\n","    age = get_age(data)\n","    sex = get_sex(data)\n","    rosc = get_rosc(data)\n","    ohca = get_ohca(data)\n","    shockable_rhythm = get_shockable_rhythm(data)\n","    ttm = get_ttm(data)\n","\n","    sex_features = np.zeros(2, dtype=int)\n","    if sex == 'Female':\n","        female = 1\n","        male   = 0\n","        other  = 0\n","    elif sex == 'Male':\n","        female = 0\n","        male   = 1\n","        other  = 0\n","    else:\n","        female = 0\n","        male   = 0\n","        other  = 1\n","\n","    features = np.array((age, female, male, other, rosc, ohca, shockable_rhythm, ttm))\n","\n","    return features\n","\n","# Extract features from the EEG data.\n","def get_eeg_features(data, sampling_frequency):\n","    num_channels, num_samples = np.shape(data)\n","\n","    if num_samples > 0:\n","        delta_psd, _ = mne.time_frequency.psd_array_welch(data, sfreq=sampling_frequency,  fmin=0.5,  fmax=8.0, verbose=False)\n","        theta_psd, _ = mne.time_frequency.psd_array_welch(data, sfreq=sampling_frequency,  fmin=4.0,  fmax=8.0, verbose=False)\n","        alpha_psd, _ = mne.time_frequency.psd_array_welch(data, sfreq=sampling_frequency,  fmin=8.0, fmax=12.0, verbose=False)\n","        beta_psd,  _ = mne.time_frequency.psd_array_welch(data, sfreq=sampling_frequency, fmin=12.0, fmax=30.0, verbose=False)\n","\n","        delta_psd_mean = np.nanmean(delta_psd, axis=1)\n","        theta_psd_mean = np.nanmean(theta_psd, axis=1)\n","        alpha_psd_mean = np.nanmean(alpha_psd, axis=1)\n","        beta_psd_mean  = np.nanmean(beta_psd,  axis=1)\n","    else:\n","        delta_psd_mean = theta_psd_mean = alpha_psd_mean = beta_psd_mean = float('nan') * np.ones(num_channels)\n","\n","    features = np.array((delta_psd_mean, theta_psd_mean, alpha_psd_mean, beta_psd_mean)).T\n","\n","    return features\n","\n","# Extract features from the ECG data.\n","def get_ecg_features(data):\n","    num_channels, num_samples = np.shape(data)\n","\n","    if num_samples > 0:\n","        mean = np.mean(data, axis=1)\n","        std  = np.std(data, axis=1)\n","    elif num_samples == 1:\n","        mean = np.mean(data, axis=1)\n","        std  = float('nan') * np.ones(num_channels)\n","    else:\n","        mean = float('nan') * np.ones(num_channels)\n","        std = float('nan') * np.ones(num_channels)\n","\n","    features = np.array((mean, std)).T\n","\n","    return features"],"metadata":{"id":"IiyPvYyo7Sin"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **TeamCode Helper for Inference**"],"metadata":{"id":"YKtP3CtW_1_k"}},{"cell_type":"code","source":["def load_challenge_models(model_folder, verbose):\n","    filename = os.path.join(model_folder, 'models.sav')\n","    return joblib.load(filename)\n","\n","# Run your trained models. This function is *required*. You should edit this function to add your code, but do *not* change the\n","# arguments of this function.\n","def run_challenge_models(models, data_folder, patient_id, verbose):\n","    imputer = models['imputer']\n","    outcome_model = models['outcome_model']\n","    cpc_model = models['cpc_model']\n","\n","    # Extract features.\n","    features = get_features(data_folder, patient_id)\n","    features = features.reshape(1, -1)\n","\n","    # Impute missing data.\n","    features = imputer.transform(features)\n","\n","    # Apply models to features.\n","    outcome = outcome_model.predict(features)[0]\n","    outcome_probability = outcome_model.predict_proba(features)[0, 1]\n","    cpc = cpc_model.predict(features)[0]\n","\n","    # Ensure that the CPC score is between (or equal to) 1 and 5.\n","    cpc = np.clip(cpc, 1, 5)\n","\n","    return outcome, outcome_probability, cpc"],"metadata":{"id":"hPpCmw3s_5-3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Generating Train/val/test folders**"],"metadata":{"id":"nCTlir2tf0fU"}},{"cell_type":"code","source":["data_folder = f\"/content/{dataset_folder_name}/training\" # input\n","patientId_label_dic = {}\n","\n","patient_id_list = find_data_folders(data_folder)\n","for patient_id in patient_id_list:\n","  patient_folder = os.path.join(data_folder, patient_id)\n","\n","  ## find label\n","  patient_metadata_file = os.path.join(data_folder, patient_id, patient_id + '.txt')\n","  patient_metadata = load_text_file(patient_metadata_file)\n","  label = get_outcome(patient_metadata)\n","\n","  patientId_label_dic[patient_folder] = label"],"metadata":{"id":"7ge48srRrgvs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# convert dictionary to pandas dataframe\n","patient_label_df = pd.DataFrame(list(patientId_label_dic.items()), columns=['patient_data_path', 'label'])"],"metadata":{"id":"sDSDSJJfugoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed = 5\n","patient_label_df = patient_label_df.sample(frac=1, random_state=seed)"],"metadata":{"id":"7znn-_Pwu1dr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Split the original dataset into train and test sets\n","train_val_df, test_df = train_test_split(patient_label_df, test_size=0.2, stratify=patient_label_df['label'], random_state=seed)\n","\n","# Split the training set into train and validation sets\n","train_df, val_df = train_test_split(train_val_df, test_size=0.2, stratify=train_val_df['label'], random_state=seed)"],"metadata":{"id":"0W_bBfZeuqvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# patient_label_df['label'].value_counts()\n","# train_df['label'].value_counts()\n","# val_df['label'].value_counts()\n","# test_df['label'].value_counts()"],"metadata":{"id":"kc46ks4MudZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = \"/content/DATA\"\n","train_val_path = f\"{data_path}/train_val\"\n","train_path = f\"{data_path}/train\"\n","val_path = f\"{data_path}/val\"\n","test_path = f\"{data_path}/test\"\n","\n","!mkdir -p $data_path\n","!mkdir -p $train_val_path\n","!mkdir -p $train_path\n","!mkdir -p $val_path\n","!mkdir -p $test_path"],"metadata":{"id":"94noCpEYwt-J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_folders(path):\n","    folder_count = 0\n","\n","    for _, dirs, _ in os.walk(path):\n","        folder_count += len(dirs)\n","\n","    return folder_count"],"metadata":{"id":"8fX0SWJY3T1q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for index, row in train_val_df.iterrows():\n","#     patient_data_path = row['patient_data_path']\n","#     !ln -s $patient_data_path $train_val_path\n","\n","\n","\n","# assert count_folders(train_val_path) == len(train_val_df)\n","\n","for index, row in train_df.iterrows():\n","    patient_data_path = row['patient_data_path']\n","    !ln -s $patient_data_path $train_path\n","\n","assert count_folders(train_path) == len(train_df)\n","\n","\n","# for index, row in val_df.iterrows():\n","#     patient_data_path = row['patient_data_path']\n","#     !ln -s $patient_data_path $val_path\n","\n","# assert count_folders(val_path) == len(val_df)"],"metadata":{"id":"fYL-QQSS1LdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for index, row in test_df.iterrows():\n","    patient_data_path = row['patient_data_path']\n","    !ln -s $patient_data_path $test_path\n","\n","assert count_folders(test_path) == len(test_df)"],"metadata":{"id":"Frc3h8nP1i5p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Train Model**"],"metadata":{"id":"uNrSdEfZebub"}},{"cell_type":"code","source":["# python train_model.py training_data model"],"metadata":{"id":"k8uEXS_qednz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_folder = train_path\n","model_folder = \"/content/model\""],"metadata":{"id":"DDxT_YBMemgj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["patient_ids = find_data_folders(data_folder)\n","num_patients = len(patient_ids)\n","\n","if num_patients==0:\n","    raise FileNotFoundError('No data was provided.')\n","\n","# Create a folder for the model if it does not already exist.\n","os.makedirs(model_folder, exist_ok=True)\n","\n","# Extract the features and labels.\n","print('Extracting features and labels from the Challenge data...')\n","\n","features = list()\n","outcomes = list()\n","cpcs = list()\n","\n","for i in range(num_patients):\n","    print('    {}/{}...'.format(i+1, num_patients))\n","\n","    current_features = get_features(data_folder, patient_ids[i])\n","    features.append(current_features)\n","\n","    # Extract labels.\n","    patient_metadata = load_challenge_data(data_folder, patient_ids[i])\n","    current_outcome = get_outcome(patient_metadata)\n","    outcomes.append(current_outcome)\n","    current_cpc = get_cpc(patient_metadata)\n","    cpcs.append(current_cpc)\n","\n","features = np.vstack(features)\n","outcomes = np.vstack(outcomes)\n","cpcs = np.vstack(cpcs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o96lB61h7Cyn","executionInfo":{"status":"ok","timestamp":1688066956714,"user_tz":-210,"elapsed":1305796,"user":{"displayName":"AIA Team","userId":"16477440184016990512"}},"outputId":"11ef5ebd-ea24-4e48-f2e8-e18a42bb1ad5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting features and labels from the Challenge data...\n","    1/388...\n","    2/388...\n","    3/388...\n","    4/388...\n","    5/388...\n","    6/388...\n","    7/388...\n","    8/388...\n","    9/388...\n","    10/388...\n","    11/388...\n","    12/388...\n","    13/388...\n","    14/388...\n","    15/388...\n","    16/388...\n","    17/388...\n","    18/388...\n","    19/388...\n","    20/388...\n","    21/388...\n","    22/388...\n","    23/388...\n","    24/388...\n","    25/388...\n","    26/388...\n","    27/388...\n","    28/388...\n","    29/388...\n","    30/388...\n","    31/388...\n","    32/388...\n","    33/388...\n","    34/388...\n","    35/388...\n","    36/388...\n","    37/388...\n","    38/388...\n","    39/388...\n","    40/388...\n","    41/388...\n","    42/388...\n","    43/388...\n","    44/388...\n","    45/388...\n","    46/388...\n","    47/388...\n","    48/388...\n","    49/388...\n","    50/388...\n","    51/388...\n","    52/388...\n","    53/388...\n","    54/388...\n","    55/388...\n","    56/388...\n","    57/388...\n","    58/388...\n","    59/388...\n","    60/388...\n","    61/388...\n","    62/388...\n","    63/388...\n","    64/388...\n","    65/388...\n","    66/388...\n","    67/388...\n","    68/388...\n","    69/388...\n","    70/388...\n","    71/388...\n","    72/388...\n","    73/388...\n","    74/388...\n","    75/388...\n","    76/388...\n","    77/388...\n","    78/388...\n","    79/388...\n","    80/388...\n","    81/388...\n","    82/388...\n","    83/388...\n","    84/388...\n","    85/388...\n","    86/388...\n","    87/388...\n","    88/388...\n","    89/388...\n","    90/388...\n","    91/388...\n","    92/388...\n","    93/388...\n","    94/388...\n","    95/388...\n","    96/388...\n","    97/388...\n","    98/388...\n","    99/388...\n","    100/388...\n","    101/388...\n","    102/388...\n","    103/388...\n","    104/388...\n","    105/388...\n","    106/388...\n","    107/388...\n","    108/388...\n","    109/388...\n","    110/388...\n","    111/388...\n","    112/388...\n","    113/388...\n","    114/388...\n","    115/388...\n","    116/388...\n","    117/388...\n","    118/388...\n","    119/388...\n","    120/388...\n","    121/388...\n","    122/388...\n","    123/388...\n","    124/388...\n","    125/388...\n","    126/388...\n","    127/388...\n","    128/388...\n","    129/388...\n","    130/388...\n","    131/388...\n","    132/388...\n","    133/388...\n","    134/388...\n","    135/388...\n","    136/388...\n","    137/388...\n","    138/388...\n","    139/388...\n","    140/388...\n","    141/388...\n","    142/388...\n","    143/388...\n","    144/388...\n","    145/388...\n","    146/388...\n","    147/388...\n","    148/388...\n","    149/388...\n","    150/388...\n","    151/388...\n","    152/388...\n","    153/388...\n","    154/388...\n","    155/388...\n","    156/388...\n","    157/388...\n","    158/388...\n","    159/388...\n","    160/388...\n","    161/388...\n","    162/388...\n","    163/388...\n","    164/388...\n","    165/388...\n","    166/388...\n","    167/388...\n","    168/388...\n","    169/388...\n","    170/388...\n","    171/388...\n","    172/388...\n","    173/388...\n","    174/388...\n","    175/388...\n","    176/388...\n","    177/388...\n","    178/388...\n","    179/388...\n","    180/388...\n","    181/388...\n","    182/388...\n","    183/388...\n","    184/388...\n","    185/388...\n","    186/388...\n","    187/388...\n","    188/388...\n","    189/388...\n","    190/388...\n","    191/388...\n","    192/388...\n","    193/388...\n","    194/388...\n","    195/388...\n","    196/388...\n","    197/388...\n","    198/388...\n","    199/388...\n","    200/388...\n","    201/388...\n","    202/388...\n","    203/388...\n","    204/388...\n","    205/388...\n","    206/388...\n","    207/388...\n","    208/388...\n","    209/388...\n","    210/388...\n","    211/388...\n","    212/388...\n","    213/388...\n","    214/388...\n","    215/388...\n","    216/388...\n","    217/388...\n","    218/388...\n","    219/388...\n","    220/388...\n","    221/388...\n","    222/388...\n","    223/388...\n","    224/388...\n","    225/388...\n","    226/388...\n","    227/388...\n","    228/388...\n","    229/388...\n","    230/388...\n","    231/388...\n","    232/388...\n","    233/388...\n","    234/388...\n","    235/388...\n","    236/388...\n","    237/388...\n","    238/388...\n","    239/388...\n","    240/388...\n","    241/388...\n","    242/388...\n","    243/388...\n","    244/388...\n","    245/388...\n","    246/388...\n","    247/388...\n","    248/388...\n","    249/388...\n","    250/388...\n","    251/388...\n","    252/388...\n","    253/388...\n","    254/388...\n","    255/388...\n","    256/388...\n","    257/388...\n","    258/388...\n","    259/388...\n","    260/388...\n","    261/388...\n","    262/388...\n","    263/388...\n","    264/388...\n","    265/388...\n","    266/388...\n","    267/388...\n","    268/388...\n","    269/388...\n","    270/388...\n","    271/388...\n","    272/388...\n","    273/388...\n","    274/388...\n","    275/388...\n","    276/388...\n","    277/388...\n","    278/388...\n","    279/388...\n","    280/388...\n","    281/388...\n","    282/388...\n","    283/388...\n","    284/388...\n","    285/388...\n","    286/388...\n","    287/388...\n","    288/388...\n","    289/388...\n","    290/388...\n","    291/388...\n","    292/388...\n","    293/388...\n","    294/388...\n","    295/388...\n","    296/388...\n","    297/388...\n","    298/388...\n","    299/388...\n","    300/388...\n","    301/388...\n","    302/388...\n","    303/388...\n","    304/388...\n","    305/388...\n","    306/388...\n","    307/388...\n","    308/388...\n","    309/388...\n","    310/388...\n","    311/388...\n","    312/388...\n","    313/388...\n","    314/388...\n","    315/388...\n","    316/388...\n","    317/388...\n","    318/388...\n","    319/388...\n","    320/388...\n","    321/388...\n","    322/388...\n","    323/388...\n","    324/388...\n","    325/388...\n","    326/388...\n","    327/388...\n","    328/388...\n","    329/388...\n","    330/388...\n","    331/388...\n","    332/388...\n","    333/388...\n","    334/388...\n","    335/388...\n","    336/388...\n","    337/388...\n","    338/388...\n","    339/388...\n","    340/388...\n","    341/388...\n","    342/388...\n","    343/388...\n","    344/388...\n","    345/388...\n","    346/388...\n","    347/388...\n","    348/388...\n","    349/388...\n","    350/388...\n","    351/388...\n","    352/388...\n","    353/388...\n","    354/388...\n","    355/388...\n","    356/388...\n","    357/388...\n","    358/388...\n","    359/388...\n","    360/388...\n","    361/388...\n","    362/388...\n","    363/388...\n","    364/388...\n","    365/388...\n","    366/388...\n","    367/388...\n","    368/388...\n","    369/388...\n","    370/388...\n","    371/388...\n","    372/388...\n","    373/388...\n","    374/388...\n","    375/388...\n","    376/388...\n","    377/388...\n","    378/388...\n","    379/388...\n","    380/388...\n","    381/388...\n","    382/388...\n","    383/388...\n","    384/388...\n","    385/388...\n","    386/388...\n","    387/388...\n","    388/388...\n"]}]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n","\n","# Train the models.\n","print('Training the Challenge model on the Challenge data...')\n","\n","# Define parameters for random forest classifier and regressor.\n","n_estimators   = 123  # Number of trees in the forest.\n","max_leaf_nodes = 456  # Maximum number of leaf nodes in each tree.\n","random_state   = 789  # Random state; set for reproducibility.\n","\n","# Impute any missing features; use the mean value by default.\n","imputer = SimpleImputer().fit(features)\n","\n","# Train the models.\n","features = imputer.transform(features)\n","outcome_model = RandomForestClassifier(\n","    n_estimators=n_estimators, max_leaf_nodes=max_leaf_nodes, random_state=random_state).fit(features, outcomes.ravel())\n","cpc_model = RandomForestRegressor(\n","    n_estimators=n_estimators, max_leaf_nodes=max_leaf_nodes, random_state=random_state).fit(features, cpcs.ravel())\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"znVwT6kyewmE","executionInfo":{"status":"ok","timestamp":1688066957539,"user_tz":-210,"elapsed":854,"user":{"displayName":"AIA Team","userId":"16477440184016990512"}},"outputId":"03d41602-2549-4465-b3e9-2538eec9a52f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training the Challenge model on the Challenge data...\n"]}]},{"cell_type":"code","source":["# Save the models.\n","save_challenge_model(model_folder, imputer, outcome_model, cpc_model)"],"metadata":{"id":"XbBH_d8D8V0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Inference on test data**"],"metadata":{"id":"0mPkYVvk-W1F"}},{"cell_type":"code","source":["# python run_model.py model test_data test_outputs"],"metadata":{"id":"ImNZmPaT-abF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_folder = test_path\n","output_folder = \"/content/output\""],"metadata":{"id":"w_7G_dyU-gJs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.makedirs(output_folder, exist_ok=True)"],"metadata":{"id":"jbq1_cPq_En-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load model(s).\n","print('Loading the Challenge models...')\n","\n","# You can use this function to perform tasks, such as loading your models, that you only need to perform once.\n","models = load_challenge_models(model_folder, True) ### Teams: Implement this function!!!\n","\n","# Find the Challenge data.\n","print('Finding the Challenge data...')\n","\n","patient_ids = find_data_folders(data_folder)\n","num_patients = len(patient_ids)\n","\n","if num_patients==0:\n","    raise Exception('No data were provided.')\n","\n","# Create a folder for the Challenge outputs if it does not already exist.\n","os.makedirs(output_folder, exist_ok=True)\n","\n","# Run the team's model(s) on the Challenge data.\n","print('Running the Challenge models on the Challenge data...')\n","\n","# Iterate over the patients.\n","for i in range(num_patients):\n","    print('    {}/{}...'.format(i+1, num_patients))\n","\n","    patient_id = patient_ids[i]\n","\n","    # Allow or disallow the model(s) to fail on parts of the data; this can be helpful for debugging.\n","    try:\n","        outcome_binary, outcome_probability, cpc = run_challenge_models(models, data_folder, patient_id, True) ### Teams: Implement this function!!!\n","    except:\n","            print('... failed.')\n","            raise\n","\n","    # Save Challenge outputs.\n","    os.makedirs(os.path.join(output_folder, patient_id), exist_ok=True)\n","    output_file = os.path.join(output_folder, patient_id, patient_id + '.txt')\n","    save_challenge_outputs(output_file, patient_id, outcome_binary, outcome_probability, cpc)\n","\n","print('Done.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m3RQ01nF_P2-","executionInfo":{"status":"ok","timestamp":1688067375298,"user_tz":-210,"elapsed":417241,"user":{"displayName":"AIA Team","userId":"16477440184016990512"}},"outputId":"41bd9aea-4798-4ec6-9442-2c3942376d48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading the Challenge models...\n","Finding the Challenge data...\n","Running the Challenge models on the Challenge data...\n","    1/122...\n","    2/122...\n","    3/122...\n","    4/122...\n","    5/122...\n","    6/122...\n","    7/122...\n","    8/122...\n","    9/122...\n","    10/122...\n","    11/122...\n","    12/122...\n","    13/122...\n","    14/122...\n","    15/122...\n","    16/122...\n","    17/122...\n","    18/122...\n","    19/122...\n","    20/122...\n","    21/122...\n","    22/122...\n","    23/122...\n","    24/122...\n","    25/122...\n","    26/122...\n","    27/122...\n","    28/122...\n","    29/122...\n","    30/122...\n","    31/122...\n","    32/122...\n","    33/122...\n","    34/122...\n","    35/122...\n","    36/122...\n","    37/122...\n","    38/122...\n","    39/122...\n","    40/122...\n","    41/122...\n","    42/122...\n","    43/122...\n","    44/122...\n","    45/122...\n","    46/122...\n","    47/122...\n","    48/122...\n","    49/122...\n","    50/122...\n","    51/122...\n","    52/122...\n","    53/122...\n","    54/122...\n","    55/122...\n","    56/122...\n","    57/122...\n","    58/122...\n","    59/122...\n","    60/122...\n","    61/122...\n","    62/122...\n","    63/122...\n","    64/122...\n","    65/122...\n","    66/122...\n","    67/122...\n","    68/122...\n","    69/122...\n","    70/122...\n","    71/122...\n","    72/122...\n","    73/122...\n","    74/122...\n","    75/122...\n","    76/122...\n","    77/122...\n","    78/122...\n","    79/122...\n","    80/122...\n","    81/122...\n","    82/122...\n","    83/122...\n","    84/122...\n","    85/122...\n","    86/122...\n","    87/122...\n","    88/122...\n","    89/122...\n","    90/122...\n","    91/122...\n","    92/122...\n","    93/122...\n","    94/122...\n","    95/122...\n","    96/122...\n","    97/122...\n","    98/122...\n","    99/122...\n","    100/122...\n","    101/122...\n","    102/122...\n","    103/122...\n","    104/122...\n","    105/122...\n","    106/122...\n","    107/122...\n","    108/122...\n","    109/122...\n","    110/122...\n","    111/122...\n","    112/122...\n","    113/122...\n","    114/122...\n","    115/122...\n","    116/122...\n","    117/122...\n","    118/122...\n","    119/122...\n","    120/122...\n","    121/122...\n","    122/122...\n","Done.\n"]}]},{"cell_type":"markdown","source":["# **Evaluation Helper**"],"metadata":{"id":"hegow6PdJfQZ"}},{"cell_type":"code","source":["#!/usr/bin/env python\n","\n","# Do *not* edit this script.\n","# These are helper functions that you can use with your code.\n","# Check the example code to see how to import these functions to your code.\n","\n","import os, numpy as np, scipy as sp, scipy.io\n","\n","### Challenge data I/O functions\n","\n","# Find the folders with data files.\n","def find_data_folders(root_folder):\n","    data_folders = list()\n","    for x in sorted(os.listdir(root_folder)):\n","        data_folder = os.path.join(root_folder, x)\n","        if os.path.isdir(data_folder):\n","            data_file = os.path.join(data_folder, x + '.txt')\n","            if os.path.isfile(data_file):\n","                data_folders.append(x)\n","    return sorted(data_folders)\n","\n","# Load the patient metadata: age, sex, etc.\n","def load_challenge_data(data_folder, patient_id):\n","    patient_metadata_file = os.path.join(data_folder, patient_id, patient_id + '.txt')\n","    patient_metadata = load_text_file(patient_metadata_file)\n","    return patient_metadata\n","\n","# Find the record names.\n","def find_recording_files(data_folder, patient_id):\n","    record_names = list()\n","    patient_folder = os.path.join(data_folder, patient_id)\n","    for file_name in sorted(os.listdir(patient_folder)):\n","        if not file_name.startswith('.') and file_name.endswith('.hea'):\n","            root, ext = os.path.splitext(file_name)\n","            record_name = '_'.join(root.split('_')[:-1])\n","            record_names.append(record_name)\n","    return sorted(record_names)\n","\n","# Load the WFDB data for the Challenge (but not all possible WFDB files).\n","def load_recording_data(record_name, check_values=True):\n","    # Allow either the record name or the header filename.\n","    root, ext = os.path.splitext(record_name)\n","    if ext=='':\n","        header_file = record_name + '.hea'\n","    else:\n","        header_file = record_name\n","\n","    # Load the header file.\n","    if not os.path.isfile(header_file):\n","        raise FileNotFoundError('{} recording not found.'.format(record_name))\n","\n","    with open(header_file, 'r') as f:\n","        header = [l.strip() for l in f.readlines() if l.strip()]\n","\n","    # Parse the header file.\n","    record_name = None\n","    num_signals = None\n","    sampling_frequency = None\n","    num_samples = None\n","    signal_files = list()\n","    gains = list()\n","    offsets = list()\n","    channels = list()\n","    initial_values = list()\n","    checksums = list()\n","\n","    for i, l in enumerate(header):\n","        arrs = [arr.strip() for arr in l.split(' ')]\n","        # Parse the record line.\n","        if i==0:\n","            record_name = arrs[0]\n","            num_signals = int(arrs[1])\n","            sampling_frequency = float(arrs[2])\n","            num_samples = int(arrs[3])\n","        # Parse the signal specification lines.\n","        elif not l.startswith('#') or len(l.strip()) == 0:\n","            signal_file = arrs[0]\n","            gain = float(arrs[2].split('/')[0])\n","            offset = int(arrs[4])\n","            initial_value = int(arrs[5])\n","            checksum = int(arrs[6])\n","            channel = arrs[8]\n","            signal_files.append(signal_file)\n","            gains.append(gain)\n","            offsets.append(offset)\n","            initial_values.append(initial_value)\n","            checksums.append(checksum)\n","            channels.append(channel)\n","\n","    # Check that the header file only references one signal file. WFDB format allows for multiple signal files, but, for\n","    # simplicity, we have not done that here.\n","    num_signal_files = len(set(signal_files))\n","    if num_signal_files!=1:\n","        raise NotImplementedError('The header file {}'.format(header_file) \\\n","            + ' references {} signal files; one signal file expected.'.format(num_signal_files))\n","\n","    # Load the signal file.\n","    head, tail = os.path.split(header_file)\n","    signal_file = os.path.join(head, list(signal_files)[0])\n","    data = np.asarray(sp.io.loadmat(signal_file)['val'])\n","\n","    # Check that the dimensions of the signal data in the signal file is consistent with the dimensions for the signal data given\n","    # in the header file.\n","    num_channels = len(channels)\n","    if np.shape(data)!=(num_channels, num_samples):\n","        raise ValueError('The header file {}'.format(header_file) \\\n","            + ' is inconsistent with the dimensions of the signal file.')\n","\n","    # Check that the initial value and checksums in the signal file are consistent with the initial value and checksums in the\n","    # header file.\n","    if check_values:\n","        for i in range(num_channels):\n","            if data[i, 0]!=initial_values[i]:\n","                raise ValueError('The initial value in header file {}'.format(header_file) \\\n","                    + ' is inconsistent with the initial value for channel {} in the signal data'.format(channels[i]))\n","            if np.sum(data[i, :])!=checksums[i]:\n","                raise ValueError('The checksum in header file {}'.format(header_file) \\\n","                    + ' is inconsistent with the checksum value for channel {} in the signal data'.format(channels[i]))\n","\n","    # Rescale the signal data using the gains and offsets.\n","    rescaled_data = np.zeros(np.shape(data), dtype=np.float32)\n","    for i in range(num_channels):\n","        rescaled_data[i, :] = (data[i, :]-offsets[i])/gains[i]\n","\n","    return rescaled_data, channels, sampling_frequency\n","\n","# Choose the channels.\n","def reduce_channels(current_data, current_channels, requested_channels):\n","    if current_channels == requested_channels:\n","        reduced_data = current_data\n","        reduced_channels = current_channels\n","    else:\n","        reduced_indices = [current_channels.index(channel) for channel in requested_channels if channel in current_channels]\n","        reduced_channels = [current_channels[i] for i in reduced_indices]\n","        reduced_data = current_data[reduced_indices, :]\n","    return reduced_data, reduced_channels\n","\n","# Choose the channels.\n","def expand_channels(current_data, current_channels, requested_channels):\n","    if current_channels == requested_channels:\n","        expanded_data = current_data\n","    else:\n","        num_current_channels, num_samples = np.shape(current_data)\n","        num_requested_channels = len(requested_channels)\n","        expanded_data = np.zeros((num_requested_channels, num_samples))\n","        for i, channel in enumerate(requested_channels):\n","            if channel in current_channels:\n","                j = current_channels.index(channel)\n","                expanded_data[i, :] = current_data[j, :]\n","            else:\n","                expanded_data[i, :] = float('nan')\n","    return expanded_data\n","\n","### Helper Challenge data I/O functions\n","\n","# Load text file as a string.\n","def load_text_file(filename):\n","    with open(filename, 'r') as f:\n","        data = f.read()\n","    return data\n","\n","# Get a variable from the patient metadata.\n","def get_variable(text, variable_name, variable_type):\n","    variable = None\n","    for l in text.split('\\n'):\n","        if l.startswith(variable_name):\n","            variable = ':'.join(l.split(':')[1:]).strip()\n","            variable = cast_variable(variable, variable_type)\n","            return variable\n","\n","# Get the patient ID variable from the patient data.\n","def get_patient_id(string):\n","    return get_variable(string, 'Patient', str)\n","\n","# Get the patient ID variable from the patient data.\n","def get_hospital(string):\n","    return get_variable(string, 'Hospital', str)\n","\n","# Get the age variable (in years) from the patient data.\n","def get_age(string):\n","    return get_variable(string, 'Age', int)\n","\n","# Get the sex variable from the patient data.\n","def get_sex(string):\n","    return get_variable(string, 'Sex', str)\n","\n","# Get the ROSC variable (in minutes) from the patient data.\n","def get_rosc(string):\n","    return get_variable(string, 'ROSC', int)\n","\n","# Get the OHCA variable from the patient data.\n","def get_ohca(string):\n","    return get_variable(string, 'OHCA', bool)\n","\n","# Get the shockable rhythm variable from the patient data.\n","def get_shockable_rhythm(string):\n","    return get_variable(string, 'Shockable Rhythm', bool)\n","\n","# Get the TTM variable (in Celsius) from the patient data.\n","def get_ttm(string):\n","    return get_variable(string, 'TTM', int)\n","\n","# Get the Outcome variable from the patient data.\n","def get_outcome(string):\n","    variable = get_variable(string, 'Outcome', str)\n","    if variable is None or is_nan(variable):\n","        raise ValueError('No outcome available. Is your code trying to load labels from the hidden data?')\n","    if variable == 'Good':\n","        variable = 0\n","    elif variable == 'Poor':\n","        variable = 1\n","    return variable\n","\n","# Get the Outcome probability variable from the patient data.\n","def get_outcome_probability(string):\n","    variable = sanitize_scalar_value(get_variable(string, 'Outcome Probability', str))\n","    if variable is None or is_nan(variable):\n","        raise ValueError('No outcome available. Is your code trying to load labels from the hidden data?')\n","    return variable\n","\n","# Get the CPC variable from the patient data.\n","def get_cpc(string):\n","    variable = sanitize_scalar_value(get_variable(string, 'CPC', str))\n","    if variable is None or is_nan(variable):\n","        raise ValueError('No CPC score available. Is your code trying to load labels from the hidden data?')\n","    return variable\n","\n","# Get the utility frequency (in Hertz) from the recording data.\n","def get_utility_frequency(string):\n","    return get_variable(string, '#Utility frequency', int)\n","\n","# Get the start time (in hh:mm:ss format) from the recording data.\n","def get_start_time(string):\n","    variable = get_variable(string, '#Start time', str)\n","    times = tuple(int(value) for value in variable.split(':'))\n","    return times\n","\n","# Get the end time (in hh:mm:ss format) from the recording data.\n","def get_end_time(string):\n","    variable = get_variable(string, '#End time', str)\n","    times = tuple(int(value) for value in variable.split(':'))\n","    return times\n","\n","# Convert seconds to days, hours, minutes, seconds.\n","def convert_seconds_to_hours_minutes_seconds(seconds):\n","    hours = int(seconds/3600 - 24*days)\n","    minutes = int(seconds/60 - 24*60*days - 60*hours)\n","    seconds = int(seconds - 24*3600*days - 3600*hours - 60*minutes)\n","    return hours, minutes, seconds\n","\n","# Convert hours, minutes, and seconds to seconds.\n","def convert_hours_minutes_seconds_to_seconds(hours, minutes, seconds):\n","    return 3600*hours + 60*minutes + seconds\n","\n","### Challenge label and output I/O functions\n","\n","\n","# Save the Challenge outputs for one file.\n","def save_challenge_outputs(filename, patient_id, outcome, outcome_probability, cpc):\n","    # Sanitize values, e.g., in case they are a singleton array.\n","    outcome = sanitize_boolean_value(outcome)\n","    outcome_probability = sanitize_scalar_value(outcome_probability)\n","    cpc = sanitize_scalar_value(cpc)\n","\n","    # Format Challenge outputs.\n","    patient_string = 'Patient: {}'.format(patient_id)\n","    if outcome == 0:\n","        outcome = 'Good'\n","    elif outcome == 1:\n","        outcome = 'Poor'\n","    outcome_string = 'Outcome: {}'.format(outcome)\n","    outcome_probability_string = 'Outcome Probability: {:.3f}'.format(outcome_probability)\n","    cpc_string = 'CPC: {:.3f}'.format(cast_int_if_int_else_float(cpc))\n","    output_string = patient_string + '\\n' + \\\n","        outcome_string + '\\n' + outcome_probability_string + '\\n' + cpc_string + '\\n'\n","\n","    # Write the Challenge outputs.\n","    if filename is not None:\n","        with open(filename, 'w') as f:\n","            f.write(output_string)\n","\n","    return output_string\n","\n","### Other helper functions\n","\n","# Check if a variable is a number or represents a number.\n","def is_number(x):\n","    try:\n","        float(x)\n","        return True\n","    except (ValueError, TypeError):\n","        return False\n","\n","# Check if a variable is an integer or represents an integer.\n","def is_integer(x):\n","    if is_number(x):\n","        return float(x).is_integer()\n","    else:\n","        return False\n","\n","# Check if a variable is a boolean or represents a boolean.\n","def is_boolean(x):\n","    if (is_number(x) and float(x)==0) or (remove_extra_characters(x) in ('False', 'false', 'FALSE', 'F', 'f')):\n","        return True\n","    elif (is_number(x) and float(x)==1) or (remove_extra_characters(x) in ('True', 'true', 'TRUE', 'T', 't')):\n","        return True\n","    else:\n","        return False\n","\n","# Check if a variable is a finite number or represents a finite number.\n","def is_finite_number(x):\n","    if is_number(x):\n","        return np.isfinite(float(x))\n","    else:\n","        return False\n","\n","# Check if a variable is a NaN (not a number) or represents a NaN.\n","def is_nan(x):\n","    if is_number(x):\n","        return np.isnan(float(x))\n","    else:\n","        return False\n","\n","# Remove any quotes, brackets (for singleton arrays), and/or invisible characters.\n","def remove_extra_characters(x):\n","    return str(x).replace('\"', '').replace(\"'\", \"\").replace('[', '').replace(']', '').replace(' ', '').strip()\n","\n","# Sanitize boolean values.\n","def sanitize_boolean_value(x):\n","    x = remove_extra_characters(x)\n","    if (is_number(x) and float(x)==0) or (remove_extra_characters(x) in ('False', 'false', 'FALSE', 'F', 'f')):\n","        return 0\n","    elif (is_number(x) and float(x)==1) or (remove_extra_characters(x) in ('True', 'true', 'TRUE', 'T', 't')):\n","        return 1\n","    else:\n","        return float('nan')\n","\n","# Sanitize integer values.\n","def sanitize_integer_value(x):\n","    x = remove_extra_characters(x)\n","    if is_integer(x):\n","        return int(float(x))\n","    else:\n","        return float('nan')\n","\n","# Sanitize scalar values.\n","def sanitize_scalar_value(x):\n","    x = remove_extra_characters(x)\n","    if is_number(x):\n","        return float(x)\n","    else:\n","        return float('nan')\n","\n","# Cast a value to a particular type.\n","def cast_variable(variable, variable_type, preserve_nan=True):\n","    if preserve_nan and is_nan(variable):\n","        variable = float('nan')\n","    else:\n","        if variable_type == bool:\n","            variable = sanitize_boolean_value(variable)\n","        elif variable_type == int:\n","            variable = sanitize_integer_value(variable)\n","        elif variable_type == float:\n","            variable = sanitize_scalar_value(variable)\n","        else:\n","            variable = variable_type(variable)\n","    return variable\n","\n","# Cast a value to an integer if the value is an integer, a float if the value is a non-integer float, and itself otherwise.\n","def cast_int_if_int_else_float(x):\n","    if is_integer(x):\n","        return int(float(x))\n","    elif is_number(x):\n","        return float(x)\n","    else:\n","        return x\n","\n","\n","import os, os.path, sys, numpy as np\n","\n","\n","# Evaluate the models.\n","def evaluate_model(label_folder, output_folder):\n","    # Load the labels.\n","    patient_ids = find_data_folders(label_folder)\n","    num_patients = len(patient_ids)\n","\n","    hospitals = list()\n","    label_outcomes = list()\n","    label_cpcs = list()\n","\n","    for i in range(num_patients):\n","        patient_data_file = os.path.join(label_folder, patient_ids[i], patient_ids[i] + '.txt')\n","        patient_data = load_text_file(patient_data_file)\n","\n","        hospital = get_hospital(patient_data)\n","        label_outcome = get_outcome(patient_data)\n","        label_cpc = get_cpc(patient_data)\n","\n","        hospitals.append(hospital)\n","        label_outcomes.append(label_outcome)\n","        label_cpcs.append(label_cpc)\n","\n","    # Load the model outputs.\n","    output_outcomes = list()\n","    output_outcome_probabilities = list()\n","    output_cpcs = list()\n","\n","    for i in range(num_patients):\n","        output_file = os.path.join(output_folder, patient_ids[i], patient_ids[i] + '.txt')\n","        output_data = load_text_file(output_file)\n","\n","        output_outcome = get_outcome(output_data)\n","        output_outcome_probability = get_outcome_probability(output_data)\n","        output_cpc = get_cpc(output_data)\n","\n","        output_outcomes.append(output_outcome)\n","        output_outcome_probabilities.append(output_outcome_probability)\n","        output_cpcs.append(output_cpc)\n","\n","    # Evaluate the models.\n","    challenge_score = compute_challenge_score(label_outcomes, output_outcome_probabilities, hospitals)\n","    auroc_outcomes, auprc_outcomes = compute_auc(label_outcomes, output_outcome_probabilities)\n","    accuracy_outcomes, _, _ = compute_accuracy(label_outcomes, output_outcomes)\n","    f_measure_outcomes, _, _ = compute_f_measure(label_outcomes, output_outcomes)\n","    mse_cpcs = compute_mse(label_cpcs, output_cpcs)\n","    mae_cpcs = compute_mae(label_cpcs, output_cpcs)\n","\n","    # Return the results.\n","    return challenge_score, auroc_outcomes, auprc_outcomes, accuracy_outcomes, f_measure_outcomes, mse_cpcs, mae_cpcs\n","\n","# Compute the Challenge score.\n","def compute_challenge_score(labels, outputs, hospitals):\n","    # Check the data.\n","    assert len(labels) == len(outputs)\n","\n","    # Convert the data to NumPy arrays for easier indexing.\n","    labels = np.asarray(labels, dtype=np.float64)\n","    outputs = np.asarray(outputs, dtype=np.float64)\n","\n","    # Identify the unique hospitals.\n","    unique_hospitals = sorted(set(hospitals))\n","    num_hospitals = len(unique_hospitals)\n","\n","    # Initialize a confusion matrix for each hospital.\n","    tps = np.zeros(num_hospitals)\n","    fps = np.zeros(num_hospitals)\n","    fns = np.zeros(num_hospitals)\n","    tns = np.zeros(num_hospitals)\n","\n","    # Compute the confusion matrix at each output threshold separately for each hospital.\n","    for i, hospital in enumerate(unique_hospitals):\n","        idx = [j for j, x in enumerate(hospitals) if x == hospital]\n","        current_labels = labels[idx]\n","        current_outputs = outputs[idx]\n","        num_instances = len(current_labels)\n","\n","        # Collect the unique output values as the thresholds for the positive and negative classes.\n","        thresholds = np.unique(current_outputs)\n","        thresholds = np.append(thresholds, thresholds[-1]+1)\n","        thresholds = thresholds[::-1]\n","        num_thresholds = len(thresholds)\n","\n","        idx = np.argsort(current_outputs)[::-1]\n","\n","        # Initialize the TPs, FPs, FNs, and TNs with no positive outputs.\n","        tp = np.zeros(num_thresholds)\n","        fp = np.zeros(num_thresholds)\n","        fn = np.zeros(num_thresholds)\n","        tn = np.zeros(num_thresholds)\n","\n","        tp[0] = 0\n","        fp[0] = 0\n","        fn[0] = np.sum(current_labels == 1)\n","        tn[0] = np.sum(current_labels == 0)\n","\n","        # Update the TPs, FPs, FNs, and TNs using the values at the previous threshold.\n","        k = 0\n","        for l in range(1, num_thresholds):\n","            tp[l] = tp[l-1]\n","            fp[l] = fp[l-1]\n","            fn[l] = fn[l-1]\n","            tn[l] = tn[l-1]\n","\n","            while k < num_instances and current_outputs[idx[k]] >= thresholds[l]:\n","                if current_labels[idx[k]] == 1:\n","                    tp[l] += 1\n","                    fn[l] -= 1\n","                else:\n","                    fp[l] += 1\n","                    tn[l] -= 1\n","                k += 1\n","\n","            # Compute the FPRs.\n","            fpr = np.zeros(num_thresholds)\n","            for l in range(num_thresholds):\n","                if tp[l] + fn[l] > 0:\n","                    fpr[l] = float(fp[l]) / float(tp[l] + fn[l])\n","                else:\n","                    fpr[l] = float('nan')\n","\n","            # Find the threshold such that FPR <= 0.05.\n","            max_fpr = 0.05\n","            if np.any(fpr <= max_fpr):\n","                l = max(l for l, x in enumerate(fpr) if x <= max_fpr)\n","                tps[i] = tp[l]\n","                fps[i] = fp[l]\n","                fns[i] = fn[l]\n","                tns[i] = tn[l]\n","            else:\n","                tps[i] = tp[0]\n","                fps[i] = fp[0]\n","                fns[i] = fn[0]\n","                tns[i] = tn[0]\n","\n","    # Compute the TPR at FPR <= 0.05 for each hospital.\n","    tp = np.sum(tps)\n","    fp = np.sum(fps)\n","    fn = np.sum(fns)\n","    tn = np.sum(tns)\n","\n","    if tp + fn > 0:\n","        max_tpr = tp / (tp + fn)\n","    else:\n","        max_tpr = float('nan')\n","\n","    return max_tpr\n","\n","# Compute area under the receiver operating characteristic curve (AUROC) and area under the precision recall curve (AUPRC).\n","def compute_auc(labels, outputs):\n","    assert len(labels) == len(outputs)\n","    num_instances = len(labels)\n","\n","    # Convert the data to NumPy arrays for easier indexing.\n","    labels = np.asarray(labels, dtype=np.float64)\n","    outputs = np.asarray(outputs, dtype=np.float64)\n","\n","    # Collect the unique output values as the thresholds for the positive and negative classes.\n","    thresholds = np.unique(outputs)\n","    thresholds = np.append(thresholds, thresholds[-1]+1)\n","    thresholds = thresholds[::-1]\n","    num_thresholds = len(thresholds)\n","\n","    idx = np.argsort(outputs)[::-1]\n","\n","    # Initialize the TPs, FPs, FNs, and TNs with no positive outputs.\n","    tp = np.zeros(num_thresholds)\n","    fp = np.zeros(num_thresholds)\n","    fn = np.zeros(num_thresholds)\n","    tn = np.zeros(num_thresholds)\n","\n","    tp[0] = 0\n","    fp[0] = 0\n","    fn[0] = np.sum(labels == 1)\n","    tn[0] = np.sum(labels == 0)\n","\n","    # Update the TPs, FPs, FNs, and TNs using the values at the previous threshold.\n","    i = 0\n","    for j in range(1, num_thresholds):\n","        tp[j] = tp[j-1]\n","        fp[j] = fp[j-1]\n","        fn[j] = fn[j-1]\n","        tn[j] = tn[j-1]\n","\n","        while i < num_instances and outputs[idx[i]] >= thresholds[j]:\n","            if labels[idx[i]] == 1:\n","                tp[j] += 1\n","                fn[j] -= 1\n","            else:\n","                fp[j] += 1\n","                tn[j] -= 1\n","            i += 1\n","\n","    # Compute the TPRs, TNRs, and PPVs at each threshold.\n","    tpr = np.zeros(num_thresholds)\n","    tnr = np.zeros(num_thresholds)\n","    ppv = np.zeros(num_thresholds)\n","    for j in range(num_thresholds):\n","        if tp[j] + fn[j] > 0:\n","            tpr[j] = tp[j] / (tp[j] + fn[j])\n","        else:\n","            tpr[j] = float('nan')\n","        if fp[j] + tn[j] > 0:\n","            tnr[j] = tn[j] / (fp[j] + tn[j])\n","        else:\n","            tnr[j] = float('nan')\n","        if tp[j] + fp[j] > 0:\n","            ppv[j] = tp[j] / (tp[j] + fp[j])\n","        else:\n","            ppv[j] = float('nan')\n","\n","    # Compute AUROC as the area under a piecewise linear function with TPR/sensitivity (x-axis) and TNR/specificity (y-axis) and\n","    # AUPRC as the area under a piecewise constant with TPR/recall (x-axis) and PPV/precision (y-axis).\n","    auroc = 0.0\n","    auprc = 0.0\n","    for j in range(num_thresholds-1):\n","        auroc += 0.5 * (tpr[j+1] - tpr[j]) * (tnr[j+1] + tnr[j])\n","        auprc += (tpr[j+1] - tpr[j]) * ppv[j+1]\n","\n","    return auroc, auprc\n","\n","# Construct the one-hot encoding of data for the given classes.\n","def compute_one_hot_encoding(data, classes):\n","    num_instances = len(data)\n","    num_classes = len(classes)\n","\n","    one_hot_encoding = np.zeros((num_instances, num_classes), dtype=np.bool_)\n","    unencoded_data = list()\n","    for i, x in enumerate(data):\n","        for j, y in enumerate(classes):\n","            if (x == y) or (is_nan(x) and is_nan(y)):\n","                one_hot_encoding[i, j] = 1\n","\n","    return one_hot_encoding\n","\n","# Compute the binary confusion matrix, where the columns are the expert labels and the rows are the classifier labels for the given\n","# classes.\n","def compute_confusion_matrix(labels, outputs, classes):\n","    assert np.shape(labels) == np.shape(outputs)\n","\n","    num_instances = len(labels)\n","    num_classes = len(classes)\n","\n","    A = np.zeros((num_classes, num_classes))\n","    for k in range(num_instances):\n","        for i in range(num_classes):\n","            for j in range(num_classes):\n","                if outputs[k, i] == 1 and labels[k, j] == 1:\n","                    A[i, j] += 1\n","\n","    return A\n","\n","# Construct the binary one-vs-rest confusion matrices, where the columns are the expert labels and the rows are the classifier\n","# for the given classes.\n","def compute_one_vs_rest_confusion_matrix(labels, outputs, classes):\n","    assert np.shape(labels) == np.shape(outputs)\n","\n","    num_instances = len(labels)\n","    num_classes = len(classes)\n","\n","    A = np.zeros((num_classes, 2, 2))\n","    for i in range(num_instances):\n","        for j in range(num_classes):\n","            if labels[i, j] == 1 and outputs[i, j] == 1: # TP\n","                A[j, 0, 0] += 1\n","            elif labels[i, j] == 0 and outputs[i, j] == 1: # FP\n","                A[j, 0, 1] += 1\n","            elif labels[i, j] == 1 and outputs[i, j] == 0: # FN\n","                A[j, 1, 0] += 1\n","            elif labels[i, j] == 0 and outputs[i, j] == 0: # TN\n","                A[j, 1, 1] += 1\n","\n","    return A\n","\n","# Compute accuracy.\n","def compute_accuracy(labels, outputs):\n","    # Compute the confusion matrix.\n","    classes = np.unique(np.concatenate((labels, outputs)))\n","    labels = compute_one_hot_encoding(labels, classes)\n","    outputs = compute_one_hot_encoding(outputs, classes)\n","    A = compute_confusion_matrix(labels, outputs, classes)\n","\n","    # Compute accuracy.\n","    if np.sum(A) > 0:\n","        accuracy = np.trace(A) / np.sum(A)\n","    else:\n","        accuracy = float('nan')\n","\n","    # Compute per-class accuracy.\n","    num_classes = len(classes)\n","    per_class_accuracy = np.zeros(num_classes)\n","    for i in range(num_classes):\n","        if np.sum(labels[:, i]) > 0:\n","            per_class_accuracy[i] = A[i, i] / np.sum(A[:, i])\n","        else:\n","            per_class_accuracy[i] = float('nan')\n","\n","    return accuracy, per_class_accuracy, classes\n","\n","# Compute macro F-measure.\n","def compute_f_measure(labels, outputs):\n","    # Compute confusion matrix.\n","    classes = np.unique(np.concatenate((labels, outputs)))\n","    labels = compute_one_hot_encoding(labels, classes)\n","    outputs = compute_one_hot_encoding(outputs, classes)\n","    A = compute_one_vs_rest_confusion_matrix(labels, outputs, classes)\n","\n","    num_classes = len(classes)\n","    per_class_f_measure = np.zeros(num_classes)\n","    for k in range(num_classes):\n","        tp, fp, fn, tn = A[k, 0, 0], A[k, 0, 1], A[k, 1, 0], A[k, 1, 1]\n","        if 2 * tp + fp + fn > 0:\n","            per_class_f_measure[k] = float(2 * tp) / float(2 * tp + fp + fn)\n","        else:\n","            per_class_f_measure[k] = float('nan')\n","\n","    if np.any(np.isfinite(per_class_f_measure)):\n","        macro_f_measure = np.nanmean(per_class_f_measure)\n","    else:\n","        macro_f_measure = float('nan')\n","\n","    return macro_f_measure, per_class_f_measure, classes\n","\n","# Compute mean-squared error.\n","def compute_mse(labels, outputs):\n","    assert len(labels) == len(outputs)\n","\n","    labels = np.asarray(labels, dtype=np.float64)\n","    outputs = np.asarray(outputs, dtype=np.float64)\n","    mse = np.mean((labels - outputs)**2)\n","\n","    return mse\n","\n","# Compute mean-absolute error.\n","def compute_mae(labels, outputs):\n","    assert len(labels) == len(outputs)\n","\n","    labels = np.asarray(labels, dtype=np.float64)\n","    outputs = np.asarray(outputs, dtype=np.float64)\n","    mae = np.mean(np.abs(labels - outputs))\n","\n","    return mae\n"],"metadata":{"id":"fErfpnStJj8I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Evaluation**"],"metadata":{"id":"EbhxeUHB_Tz6"}},{"cell_type":"code","source":["scores = evaluate_model(test_path, output_folder)\n","\n","# Unpack the scores.\n","challenge_score, auroc_outcomes, auprc_outcomes, accuracy_outcomes, f_measure_outcomes, mse_cpcs, mae_cpcs = scores\n","\n","# Construct a string with scores.\n","output_string = \\\n","    'Challenge Score: {:.3f}\\n'.format(challenge_score) + \\\n","    'Outcome AUROC: {:.3f}\\n'.format(auroc_outcomes) + \\\n","    'Outcome AUPRC: {:.3f}\\n'.format(auprc_outcomes) + \\\n","    'Outcome Accuracy: {:.3f}\\n'.format(accuracy_outcomes) + \\\n","    'Outcome F-measure: {:.3f}\\n'.format(f_measure_outcomes) + \\\n","    'CPC MSE: {:.3f}\\n'.format(mse_cpcs) + \\\n","    'CPC MAE: {:.3f}\\n'.format(mae_cpcs)"],"metadata":{"id":"Dq7CUgeYKh3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(output_string)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B2IJPqV2LXkB","executionInfo":{"status":"ok","timestamp":1688067399065,"user_tz":-210,"elapsed":30,"user":{"displayName":"AIA Team","userId":"16477440184016990512"}},"outputId":"be6b212a-1b33-4f1d-d046-dc3920e1d957"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Challenge Score: 0.338\n","Outcome AUROC: 0.743\n","Outcome AUPRC: 0.803\n","Outcome Accuracy: 0.713\n","Outcome F-measure: 0.677\n","CPC MSE: 2.886\n","CPC MAE: 1.421\n","\n"]}]}]}